{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7gEg4DRBwbO"
   },
   "source": [
    "\n",
    "# Overview\n",
    "This CodeLab demonstrates how to build a fused TFLite LSTM model for MNIST recognition using Keras, and how to convert it to TensorFlow Lite.\n",
    "\n",
    "The CodeLab is very similar to the Keras LSTM [CodeLab](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/keras_lstm.ipynb). However, we're creating fused LSTM ops rather than the unfused versoin.\n",
    "\n",
    "Also note: We're not trying to build the model to be a real world application, but only demonstrate how to use TensorFlow Lite. You can a build a much better model using CNN models. For a more canonical lstm codelab, please see [here](https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1v9muouWCrLA"
   },
   "source": [
    "\n",
    "# Step 0: Prerequisites\n",
    "It's recommended to try this feature with the newest TensorFlow nightly pip build."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeo4IA1xC4O8"
   },
   "source": [
    "# Step 1: Build the MNIST LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "yMtp56hRBvHe"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/path/to')\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import click\n",
    "from utils import mdrnn_config\n",
    "\n",
    "import mdrnn as mdrnn\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: s\n",
      "Units: 64\n",
      "Layers: 2\n",
      "Mixtures: 5\n",
      "Loaded perfs: 46\n",
      "Num touches: 94603\n",
      "Corpus Examples: 33\n",
      "Number of training examples:\n",
      "X: (92873, 50, 9)\n",
      "y: (92873, 50, 9)\n",
      "Building EMPI Model...\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inputs (InputLayer)         [(None, 50, 9)]           0         \n",
      "                                                                 \n",
      " lstm0 (LSTM)                (None, 50, 64)            18944     \n",
      "                                                                 \n",
      " lstm1 (LSTM)                (None, 50, 64)            33024     \n",
      "                                                                 \n",
      " td_mdn (TimeDistributed)    (None, 50, 95)            6175      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 58143 (227.12 KB)\n",
      "Trainable params: 58143 (227.12 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Enabling Early Stopping.\n",
      "Epoch 1/3\n",
      "1307/1307 [==============================] - ETA: 0s - loss: 37.3450\n",
      "Epoch 1: val_loss improved from inf to 19.93788, saving model to models/musicMDRNN-dim9-layers2-units64-mixtures5-scale10-E01-VL19.94.hdf5\n",
      "1307/1307 [==============================] - 102s 74ms/step - loss: 37.3450 - val_loss: 19.9379\n",
      "Epoch 2/3\n",
      "   1/1307 [..............................] - ETA: 2:09 - loss: 34.3674"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ninjatimtam/.cache/pypoetry/virtualenvs/impsy-0RgZpL3H-py3.11/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1306/1307 [============================>.] - ETA: 0s - loss: 22.3445\n",
      "Epoch 2: val_loss improved from 19.93788 to 19.07641, saving model to models/musicMDRNN-dim9-layers2-units64-mixtures5-scale10-E02-VL19.08.hdf5\n",
      "1307/1307 [==============================] - 102s 78ms/step - loss: 22.3445 - val_loss: 19.0764\n",
      "Epoch 3/3\n",
      "1307/1307 [==============================] - ETA: 0s - loss: 18.7162\n",
      "Epoch 3: val_loss improved from 19.07641 to 15.73800, saving model to models/musicMDRNN-dim9-layers2-units64-mixtures5-scale10-E03-VL15.74.hdf5\n",
      "1307/1307 [==============================] - 106s 81ms/step - loss: 18.7162 - val_loss: 15.7380\n"
     ]
    }
   ],
   "source": [
    "    # Model Hyperparameters\n",
    "    SEQ_LEN = 50\n",
    "    SEQ_STEP = 1\n",
    "    TIME_DIST = True\n",
    "    # Training Hyperparameters:\n",
    "    VAL_SPLIT = 0.10\n",
    "    # Set random seed for reproducibility\n",
    "    SEED = 2345\n",
    "\n",
    "    # Directly defining \"training\" function's inputs\n",
    "    dimension = 9\n",
    "    modelsize = \"s\"\n",
    "    earlystopping = True\n",
    "    patience = 10\n",
    "    numepochs = 3\n",
    "    batchsize = 64\n",
    "\n",
    "\n",
    "    model_config = mdrnn_config(modelsize)\n",
    "    mdrnn_units = model_config[\"units\"]\n",
    "    mdrnn_layers = model_config[\"layers\"]\n",
    "    mdrnn_mixes = model_config[\"mixes\"]\n",
    "\n",
    "    print(\"Model size:\", modelsize)\n",
    "    print(\"Units:\", mdrnn_units)\n",
    "    print(\"Layers:\", mdrnn_layers)\n",
    "    print(\"Mixtures:\", mdrnn_mixes)\n",
    "\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    # Load dataset\n",
    "    dataset_location = '../datasets/'\n",
    "    dataset_filename = f'training-dataset-{str(dimension)}d.npz'\n",
    "\n",
    "    with np.load(dataset_location + dataset_filename, allow_pickle=True) as loaded:\n",
    "        perfs = loaded['perfs']\n",
    "\n",
    "    print(\"Loaded perfs:\", len(perfs))\n",
    "    print(\"Num touches:\", np.sum([len(l) for l in perfs]))\n",
    "    corpus = perfs  # might need to do some processing here...processing\n",
    "    # Restrict corpus to sequences longer than the corpus.\n",
    "    corpus = [l for l in corpus if len(l) > SEQ_LEN+1]\n",
    "    print(\"Corpus Examples:\", len(corpus))\n",
    "    # Prepare training data as X and Y.\n",
    "    slices = []\n",
    "    for seq in corpus:\n",
    "        slices += mdrnn.slice_sequence_examples(seq,\n",
    "                                                    SEQ_LEN+1,\n",
    "                                                    step_size=SEQ_STEP)\n",
    "    X, y = mdrnn.seq_to_overlapping_format(slices)\n",
    "    X = np.array(X) * mdrnn.SCALE_FACTOR\n",
    "    y = np.array(y) * mdrnn.SCALE_FACTOR\n",
    "\n",
    "    print(\"Number of training examples:\")\n",
    "    print(\"X:\", X.shape)\n",
    "    print(\"y:\", y.shape)\n",
    "\n",
    "    # Setup Training Model\n",
    "    model = mdrnn.build_model(seq_len=SEQ_LEN,\n",
    "                                hidden_units=mdrnn_units,\n",
    "                                num_mixtures=mdrnn_mixes,\n",
    "                                layers=mdrnn_layers,\n",
    "                                out_dim=dimension,\n",
    "                                time_dist=TIME_DIST,\n",
    "                                inference=False,\n",
    "                                compile_model=True,\n",
    "                                print_summary=True)\n",
    "\n",
    "    model_dir = \"models/\"\n",
    "    model_name = \"musicMDRNN\" + \"-dim\" + str(dimension) + \"-layers\" + str(mdrnn_layers) + \"-units\" + str(mdrnn_units) + \"-mixtures\" + str(mdrnn_mixes) + \"-scale\" + str(mdrnn.SCALE_FACTOR)\n",
    "    date_string = datetime.datetime.today().strftime('%Y%m%d-%H_%M_%S')\n",
    "\n",
    "    filepath = model_dir + model_name + \"-E{epoch:02d}-VL{val_loss:.2f}.hdf5\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
    "                                                monitor='val_loss',\n",
    "                                                verbose=1,\n",
    "                                                save_best_only=True,\n",
    "                                                mode='min')\n",
    "    terminateOnNaN = keras.callbacks.TerminateOnNaN()\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)\n",
    "    tboard = keras.callbacks.TensorBoard(log_dir='./logs/' + date_string + model_name,\n",
    "                                        histogram_freq=0,\n",
    "                                        write_graph=True,\n",
    "                                        update_freq='epoch')\n",
    "\n",
    "    callbacks = [checkpoint, terminateOnNaN, tboard]\n",
    "    if earlystopping:\n",
    "        print(\"Enabling Early Stopping.\")\n",
    "        callbacks.append(early_stopping)\n",
    "    # Train\n",
    "    history = model.fit(X, y, batch_size=batchsize,\n",
    "                        epochs=numepochs,\n",
    "                        validation_split=VAL_SPLIT,\n",
    "                        callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IQtjKcMYC_nD"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(28, 28), name='input'),\n",
    "    tf.keras.layers.LSTM(20, time_major=False, return_sequences=True),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax, name='output')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qS_79wHVDcri"
   },
   "source": [
    "# Step 2: Train & Evaluate the model.\n",
    "We will train the model using MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yEAraXGDlcQ"
   },
   "outputs": [],
   "source": [
    "# Load MNIST dataset.\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)\n",
    "\n",
    "# Change this to True if you want to test the flow rapidly.\n",
    "# Train with a small dataset and only 1 epoch. The model will work poorly\n",
    "# but this provides a fast way to test if the conversion works end to end.\n",
    "_FAST_TRAINING = False\n",
    "_EPOCHS = 5\n",
    "if _FAST_TRAINING:\n",
    "  _EPOCHS = 1\n",
    "  _TRAINING_DATA_COUNT = 1000\n",
    "  x_train = x_train[:_TRAINING_DATA_COUNT]\n",
    "  y_train = y_train[:_TRAINING_DATA_COUNT]\n",
    "\n",
    "model.fit(x_train, y_train, epochs=_EPOCHS)\n",
    "model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pGyWlkJDpMQ"
   },
   "source": [
    "# Step 3: Convert the Keras model to TensorFlow Lite model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "tB1NZBUHDogR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: keras_lstm/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: keras_lstm/assets\n",
      "2024-05-20 18:34:34.672434: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-05-20 18:34:34.672501: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-05-20 18:34:34.672714: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: keras_lstm\n",
      "2024-05-20 18:34:34.685635: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-05-20 18:34:34.685701: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: keras_lstm\n",
      "2024-05-20 18:34:34.726090: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-05-20 18:34:34.865839: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: keras_lstm\n",
      "2024-05-20 18:34:34.960719: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 288005 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 47, Total Ops 82, % non-converted = 57.32 %\n",
      " * 47 ARITH ops\n",
      "\n",
      "- arith.constant:   47 occurrences  (f32: 36, i32: 11)\n",
      "\n",
      "  (i1: 1, i32: 1)\n",
      "\n",
      "\n",
      "  (f32: 1, i32: 2)\n",
      "  (f32: 2, i32: 2)\n",
      "  (f32: 1)\n",
      "  (f32: 3)\n",
      "  (f32: 1)\n",
      "  (i1: 1)\n",
      "\n",
      "  (f32: 1, i32: 2)\n",
      "  (f32: 2)\n",
      "  (f32: 2)\n",
      "  (f32: 2)\n",
      "  (i32: 1)\n",
      "\n",
      "2024-05-20 18:34:35.162320: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2989] Estimated count of arithmetic ops: 0.787 M  ops, equivalently 0.394 M  MACs\n"
     ]
    }
   ],
   "source": [
    "run_model = tf.function(lambda x: model(x))\n",
    "# This is important, let's fix the input size.\n",
    "BATCH_SIZE = batchsize\n",
    "STEPS = SEQ_LEN\n",
    "INPUT_SIZE = dimension \n",
    "concrete_func = run_model.get_concrete_function(\n",
    "    tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], model.inputs[0].dtype))\n",
    "\n",
    "# model directory.\n",
    "MODEL_DIR = \"keras_lstm\"\n",
    "model.save(MODEL_DIR, save_format=\"tf\", signatures=concrete_func)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INFyl-J3FAOY"
   },
   "source": [
    "# Step 4: Check the converted TensorFlow Lite model.\n",
    "Now load the TensorFlow Lite model and use the TensorFlow Lite python interpreter to verify the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0-b0IKK2FGuO"
   },
   "outputs": [],
   "source": [
    "# Run the model with TensorFlow to get expected results.\n",
    "TEST_CASES = 10\n",
    "\n",
    "# Run the model with TensorFlow Lite\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "for i in range(TEST_CASES):\n",
    "  expected = model.predict(x_test[i:i+1])\n",
    "  interpreter.set_tensor(input_details[0][\"index\"], x_test[i:i+1, :, :])\n",
    "  interpreter.invoke()\n",
    "  result = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "\n",
    "  # Assert if the result of TFLite model is consistent with the TF model.\n",
    "  np.testing.assert_almost_equal(expected, result, decimal=5)\n",
    "  print(\"Done. The result of TensorFlow matches the result of TensorFlow Lite.\")\n",
    "\n",
    "  # Please note: TfLite fused Lstm kernel is stateful, so we need to reset\n",
    "  # the states.\n",
    "  # Clean up internal states.\n",
    "  interpreter.reset_all_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cf6KC9fbFY5f"
   },
   "source": [
    "# Step 5: Let's inspect the converted TFLite model.\n",
    "\n",
    "Let's check the model, you can see the LSTM will be in it's fused format.\n",
    "\n",
    "![Fused LSTM](https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/lite/examples/experimental_new_converter/keras_lstm.png)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Keras LSTM fusion Codelab.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
